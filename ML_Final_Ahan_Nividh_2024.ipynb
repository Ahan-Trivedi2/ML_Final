{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Final Project: RL Basics and Tic-Tac-Toe w/ Q-learning and Deep Q-Learning\n",
        "Ahan Trivedi and Nividh Singh -\n",
        "Machine Learning, Olin College Fall 2024\n"
      ],
      "metadata": {
        "id": "gQuDYy1lWJEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of contents"
      ],
      "metadata": {
        "id": "j4boGTM4sEu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Project Introduction and Learning Goals\n",
        "2. Introduction to Reinforcement Learning\n",
        "3. Markov Decision Process\n",
        "4. General Reinforcment Learning Concepts\n",
        "5. Q-Learning (Code Implementation)\n",
        "6. Deep Q-Learning (Code Implementation)\n",
        "7. Sources"
      ],
      "metadata": {
        "id": "adbMKeylsMUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Project Introduction and Learning Goals"
      ],
      "metadata": {
        "id": "QKUpqFSxXLp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented a Q-network and a Double Deep Q-Network (DDQN) using PyTorch to train an agent that can play and win tic-tac-toe by maximizing cumulative rewards through efficient decision-making. We chose  to focus on reinforcement learning for our final project because we didn’t cover it in class and it has a really broad range of applications in solving real-world problems. We chose to build a model for tic-tac-toe because it doesn’t stretch compute power as much as other models, allowing us to delve deep into the theory. We hope that this Jupyter notebook can give you guys a basic understanding of RL and more specifically, introduce Q-Learning. This notebook will be heavier on the theory rather than implementation, since we think RL implementation is easy enough to understand with a solid foundation in theory."
      ],
      "metadata": {
        "id": "j9gwrySeXOs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text here](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "7WoiN1CjpwZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Introduction to Reinforcement Learning"
      ],
      "metadata": {
        "id": "F4jFAn44YL2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning is a machine learning technique where an agent learns to make decisions by interacting with an environment and getting rewards based on the state that the agent reaches. The goal is to give the model the bare minimum amount of information, and for the model to figure out how to maximize reward over a long period of time.\n",
        "\n",
        "\n",
        "  - Agent: The learner that interacts with the environment (e.g., the QN/DDQN (model) which controls what move the computer plays against you in tic-tac-toe)[2].\n",
        "  - Environment: The world where the agent acts [2]\n",
        "  - State: A snapshot of the environment at a particular moment (e.g., the tic-tac-toe board) [2].\n",
        "  - Action: The moves the agent can make (e.g., place an X or an O) [2,3].\n",
        "  - Reward: A numerical score that tells the agent how good or bad an action was (e.g. a 1 if the model wins, a -1 if the model places a letter on a preexisting letter or loses, and a 0 in all other cases) [3]\n",
        "  - Policy: A strategy that the agent uses to decide its next action based on the current state. (e.g. the method the model uses to decide the next square to play) [2]\n",
        "\n",
        "The agent’s goal is to learn a policy that *maximizes the cumulative rewards* it collects during its lifetime."
      ],
      "metadata": {
        "id": "E2RCjHN1YQC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.ytimg.com/vi/KEQhaBIZ9yk/maxresdefault.jpg\" width=\"300\" height=\"200\">"
      ],
      "metadata": {
        "id": "5TIOGWhFqF-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Markov Decision Process"
      ],
      "metadata": {
        "id": "yT59EdA4Yp2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Markov Decision Process (MDP) has the following property: Any future state is based on the current state, and not any past states [2].\n",
        "\n",
        "  - States (S): All the possible situations the agent can encounter (in tic-tac-toe, this would be all possible boards) [2]\n",
        "  - Actions (A): The choices the agent has at each state (in tic-tac-toe, there are 9 actions for the 9 possible squares to place a piece in) [2]\n",
        "  - A(s) is the set of actions that can be taken in state S [2]\n",
        "  - Rewards (R): Feedback from the environment for each action (the same as before). [2]\n",
        "  - Transitions (T): Rules that determine how actions change the state (what happens when a letter is placed down). [2]\n",
        "\n",
        "  MDPs may take a little bit to understand, so let's look at a few more examples. First, let's think about a drone. If our state contains the position of the drone, this is not a MDP because we rely on previous states to tell us which direction the drone is flying/spinning. However, if the state includes things like linear and angular velocity/acceleration, then this is an MDP because the current state has all the information.\n"
      ],
      "metadata": {
        "id": "Hpb65XF2Yt6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text here](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "RrH6ojrerrEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 General Reinforcement Learning Concepts"
      ],
      "metadata": {
        "id": "yIJvbEvzY5gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Rewards"
      ],
      "metadata": {
        "id": "4JlGCp-RZIUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent’s job is to maximize the cumulative reward it earns. One of the key trade offs is how much to value reward in the near future vs the far future. Gamma is often used to make this distinction, where every timestep in the future is discounted by a factor of gamma. If gamma is one, that means near future rewards are valued as much as far future rewards. If gamma is zero, it means that only the current reward matters to the model. In our tic-tac-toe game, we manually set the rewards for our tic-tac-toe environment to help model convergence [5]."
      ],
      "metadata": {
        "id": "3S8HPiGRZMRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cumulative reward is calculated as :$$R = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\dots$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $r_0, r_1, r_2, \\dots$ are the rewards over time.\n",
        "- $\\gamma$ is the discount factor (between 0 and 1) that controls how much the agent values future rewards [3].\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UpT0WAUXZPZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Exploration vs. Exploitation\n"
      ],
      "metadata": {
        "id": "7ButV2edZX-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another large tradeoff in our training is how much to explore vs. rely on current knowledge for the best option. Let’s say that we’re trying to find our way from LA to NYC. Let's say we have a simple reward structure of +1000 for if we reach NYC, and -1 for every hour we take. Our model starts off with zero knowledge of the environment, so it randomly explores. However, once we get to NYC, that path will always have the highest rewards [4]. However, this path could have gone through Seattle and then Florida and then Back to SF and then finally to NYC. If the model always used its knowledge to decide where to go, we’d never find a more efficient path. Thus, we need to explore new paths [4]. This idea can be applied to almost all RL applications, because there is a decent chance that the first path to a reward it finds isn’t the most optimal or the highest reward [4]."
      ],
      "metadata": {
        "id": "Q3yRif-7Zadz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg\" width=\"300\" height=\"200\">\n"
      ],
      "metadata": {
        "id": "9PqGZot6tcMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1 Epsilon Greedy Strategy"
      ],
      "metadata": {
        "id": "bVIBnUcVZgRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most common ways of managing this tradeoff is with a parameter typically noted as Epsilon [3]. This controls the probability of exploration or exploitation. At every state, there is a probability of epsilon that we chose a random action and a probability of (1 - epsilon) that we choose the optimal action [3]. Oftentimes we start with an epsilon equal to one to encourage exploration, and decay the value of epsilon as the model learns [3]."
      ],
      "metadata": {
        "id": "daa-Ya1qZk1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pylessons.com/media/Tutorials/Reinforcement-learning-tutorial/Epsilon-Greedy-DQN/Epsilon-Greedy-DQN.jpg\" width=\"300\" height=\"200\">"
      ],
      "metadata": {
        "id": "IlkGXzjx1Qnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2 Temperature"
      ],
      "metadata": {
        "id": "jdZUa5WXZnyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another method of managing exploration vs exploitation is temperature. This is similar to epsilon for high values, where the actions are chosen nearly uniformly. However, when there is a low temperature value the actions are chosen probabilistically based on their estimated value [5]. For example, if there are two actions with a reward of 1 and 2, there would be a 33% chance of selecting the first action and 67% chance of selecting the second reward."
      ],
      "metadata": {
        "id": "jlpjPRcoZrg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.3 Upper Confidence Bound (UCB) Factor"
      ],
      "metadata": {
        "id": "LhNLLVGEZwGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the last method we’ll cover is UCB. In this method, we look at a combination of estimated value and uncertainty of an action. This encourages us to explore less visited actions while also making use of known high value actions. The amount to rely on confidence vs value is often determined with a coincidence scaling parameter, c."
      ],
      "metadata": {
        "id": "l8-lsdgkZy-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Episodes and Trajectories"
      ],
      "metadata": {
        "id": "x2i_dolraSA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.1 Episodes"
      ],
      "metadata": {
        "id": "eEtW3-XUaXgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training an RL model, we often use the notion of episodes. One episode can be seen as one iteration through the environment [4]. Normally, an episode is either stopped when the agent reaches a terminal state or reaches a set amount of actions [4]. Terminal states may vary from winning a board game to crashing a car [4]. Similarly, the rationale behind a set amount of actions is to prevent getting stuck in an episode (like a game of chess can go in loops)."
      ],
      "metadata": {
        "id": "WWy1cWsJaaud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2 Trajectories"
      ],
      "metadata": {
        "id": "XwoNyj8zav6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trajectories is the sequence of states, actions and rewards the agent experienced in an episode."
      ],
      "metadata": {
        "id": "mzY6eKQCayXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Model Based vs. Model Free"
      ],
      "metadata": {
        "id": "ZjfdHbSZa0-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most RL can be broken down into model based and model free environments. In model based, agents build an internal model of the environment to predict future states and rewards, which allows for planning and simulation before taking actions [2]. On the other hand, model free means the agent learns directly from trial and error interactions with the environment. It solely bases its policy on observed outcomes [2].\n",
        "\n",
        "For example, a robot navigating a maze might use a model based approach to plan the most efficient way to get to the solution. On the other hand, a simple game like tic-tac-toe can often be learned without a model.\n"
      ],
      "metadata": {
        "id": "pA81KsjTa4Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Q-Learning"
      ],
      "metadata": {
        "id": "Rj6yWnCAcCDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train our RL model we used a process called Q-learning and Double Deep Q-Learning. But before we go into the complexity of the DDQN algorithm, it is important to understand what Q-learning is in general. Q-learning is a technique that helps the agent learn the quality (Q) of each action in a state. At each particular state of our agent, we calculate the ‘state-action’ value of the agent at that state [3]. A ‘state-action’ value is the idea of being at one state and then seeing all of the possible reward options from moving to the next state. The agent chooses the action that has the highest reward value (Q-value) [3]. Simply put, the Q-value is the “quality” of the action in terms of the reward it is bringing in. We like to think of the Q-value as “If I take this action ‘a’ in this state ‘s’, the total reward I can expect over time (immediate + future) while following the best possible policy afterward looks like….” [1]\n",
        "\n",
        "In our tic-tac-toe game environment we set the rewards for the game. A Q-table helps a learning agent decide the best moves to make based on the game state. The rows of the Q-table represent all possible board configurations (states), and the columns represent the possible actions (placing an X in any empty square). Initially, the Q-table is filled with random or zero values, as the agent knows nothing about the game. The agent then begins to play games, exploring different moves and learning from the outcomes. An example Q-table for tic-tac-toe could look like ...\n",
        "\n"
      ],
      "metadata": {
        "id": "8neLCNWHcG42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:798/0*xHowI1EteKPA69Ds.PNG\" width=\"300\" height=\"200\">"
      ],
      "metadata": {
        "id": "MDNPK3Es-jVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When the agent places an X (takes an action) on the board, the environment (opponent and game rules) responds by updating the board and potentially transitioning to a new game state. The agent receives a reward based on the outcome of the move [3]. For example, if the agent wins the game by forming a line of three Xs, it receives a reward of +1. If the game results in a loss because the opponent forms a line of three Os, the reward is -1. A draw results in a reward of 0.\n",
        "\n",
        "As the game progresses, the agent uses the Q-learning formula to update the Q-table for the current state and action based on the reward received and the predicted future rewards from the next state [1]. Over many games, the agent refines the Q-values in the table, associating higher values with actions that lead to winning states and lower values with actions that lead to losing states [3, 1]. Through this iterative process, the Q-table becomes a strategic guide, allowing the agent to make smarter moves in future games, ultimately mastering the game of Tic Tac Toe.\n",
        "\n",
        "In Q-learning we update our Q-table based on the bellman equation for Q-values $$Q(s, a) = r + \\gamma \\max_a Q(s', a)$$\n",
        " The Q-value for an action at a particular state is computed as the immediate reward for the current state action pair along with the expected future rewards from all future states starting at the next state [3]. It's recursive, so it breaks down the total future rewards into manageable steps, with each step building on the next. Simplifying this down, the current Q-value for state ‘s’ and action ‘a’ is equal to the immediate reward plus the best Q-value for the next state s′ (assuming the agent chooses the optimal action from there onward) discounted by a factor. To update the Q-value as we go through episodes of training, we use the Q-learning update equation  $$Q(s, a) = Q(s, a) + \\alpha \\left[ r + \\gamma \\max_a Q(s', a) - Q(s, a) \\right]$$ [1]\n",
        ". It is a variant of the bellman equation which shows how we update Q-values iteratively. We add the current state-action pair to the temporal difference error to update the Q-values iteratively. The reward for the next step plus the discounted maximum Q-value for the next state ‘s’ over all possible actions minus the current state action pair gives us the temporal difference (TD) error [3, 1]. This measures how much the Q-value Q(s,a) differs from the new estimate of the expected reward. We then scale this TD error and add it to the current state-action pair to determine how we are going to change the Q-value of the current state-action pair on the Q-table [3].\n",
        "\n"
      ],
      "metadata": {
        "id": "86RT8eZVcfTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Training of Q-Learning Tic-Tac-Toe Model"
      ],
      "metadata": {
        "id": "-RCo7H6LdCz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run these scripts to set up an environment and train a Q-learning model of tic-tac-toe. We commented our code in detail to help users understand what we did and how they relate back to concepts we explained in this notebook. Please read the code and the comments in detail and refer to sections above to make sure you understand every line of code."
      ],
      "metadata": {
        "id": "0iXnb76rdGKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TicTacToeEnv:\n",
        "    # Define constants for the players and blank spaces\n",
        "    X = 1  # Player 1 (X)\n",
        "    O = 2  # Player 2 (O)\n",
        "    BLANK = 0  # Blank space on the board\n",
        "\n",
        "    def __init__(self, debug=False):\n",
        "        # Initialize the game environment\n",
        "        self.reset()  # Start with a fresh board\n",
        "        self.debug = debug  # Enable or disable debug prints\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the board to an empty state\n",
        "        self.board = np.zeros(9, dtype=int)  # 1D array of 9 zeros for the board\n",
        "        self.current_player = self.X  # Player X always starts first\n",
        "        self.done = False  # Game is not over yet\n",
        "        self.winner = None  # No winner at the start\n",
        "        return self.get_state()  # Return the current state of the board\n",
        "\n",
        "    def step(self, action):\n",
        "        # Process a player's move (action)\n",
        "        reward: float\n",
        "        if self.done:\n",
        "            # Can't make a move if the game is already over\n",
        "            raise ValueError(\"Game is already done.\")\n",
        "        if self.board[action] != 0:\n",
        "            # If the move is invalid (space already occupied)\n",
        "            if self.debug:\n",
        "                print(f\"Invalid move {(self.board == 0).sum()}\", end=\"\\t\")\n",
        "            reward = -10  # Penalize for an invalid move\n",
        "            self.done = True  # End the game\n",
        "            self.winner = -self.current_player  # No valid winner, just end game\n",
        "            return self.get_state(), reward, self.done, {}\n",
        "\n",
        "        # Place the current player's mark on the board\n",
        "        self.board[action] = self.current_player\n",
        "\n",
        "        # Check if the current player has won the game\n",
        "        if TicTacToeEnv.check_winner(self.board, self.current_player):\n",
        "            if self.debug:\n",
        "                player = \"X\" if self.current_player == self.X else \"O\"\n",
        "                print(f\"{player} Wins\")\n",
        "            self.done = True  # Game is over\n",
        "            self.winner = self.current_player  # Set the winner\n",
        "            reward = 100  # Reward for winning\n",
        "        elif (self.board == 0).sum() == 0:\n",
        "            # If the board is full and no winner, it's a draw\n",
        "            if self.debug:\n",
        "                print(\"Draw Game\")\n",
        "            self.done = True  # Game is over\n",
        "            self.winner = 0  # No winner\n",
        "            reward = 0  # No reward for a draw\n",
        "        else:\n",
        "            # Valid move, but no one has won yet\n",
        "            reward = 0  # Neutral reward for a valid move\n",
        "\n",
        "        # Switch to the other player's turn\n",
        "        self.current_player = self.X if self.current_player == self.O else self.O\n",
        "\n",
        "        return self.get_state(), reward, self.done, {}\n",
        "\n",
        "    def check_draw(self):\n",
        "        # Check if the board is full (no more valid moves)\n",
        "        return (self.board == TicTacToeEnv.BLANK).sum() == 0\n",
        "\n",
        "    def check_loss(self):\n",
        "        # Check if the current player can lose in the next move\n",
        "        for i in range(9):\n",
        "            if self.board[i] == 0:  # Try every blank space\n",
        "                self.board[i] = self.current_player\n",
        "                if TicTacToeEnv.check_winner(self.board, self.current_player):\n",
        "                    return True\n",
        "                self.board[i] = 0  # Undo the move\n",
        "        return False\n",
        "\n",
        "    def get_state(self):\n",
        "        # Represent the board state as a single number\n",
        "        j = 0\n",
        "        for i in range(9):\n",
        "            j *= 3\n",
        "            j += self.board[i]\n",
        "        return j\n",
        "\n",
        "    def update_board(self, j):\n",
        "        # Update the board using a single number representation\n",
        "        for i in range(8, -1, -1):\n",
        "            self.board[i] = j % 3\n",
        "            j //= 3  # Integer division\n",
        "\n",
        "    def check_winner(board, player):\n",
        "        # Check all possible winning combinations\n",
        "        wins = [\n",
        "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Horizontal lines\n",
        "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Vertical lines\n",
        "            [0, 4, 8], [2, 4, 6]  # Diagonal lines\n",
        "        ]\n",
        "        for combo in wins:\n",
        "            if all(board[i] == player for i in combo):\n",
        "                return True  # Found a winning line\n",
        "        return False  # No winner\n",
        "\n",
        "    def valid_actions(self):\n",
        "        # Return a list of all empty spaces (valid moves)\n",
        "        return [i for i in range(9) if self.board[i] == 0]\n",
        "\n",
        "    def print_board(self):\n",
        "        # Print the current state of the board for visualization\n",
        "        symbols = {self.X: \"X\", self.O: \"O\", self.BLANK: \" \"}\n",
        "        for i in range(0, 9, 3):\n",
        "            # Create a row of the board\n",
        "            row = [symbols[self.board[i + j]] for j in range(3)]\n",
        "            print(\" | \".join(row))  # Print the row\n",
        "            if i < 6:\n",
        "                print(\"---------\")  # Print a separator for rows\n"
      ],
      "metadata": {
        "id": "S6cphQaDGPI8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "# from ticTacToeQlearningEnv import TicTacToeEnv\n",
        "\n",
        "# Define the environment\n",
        "n_states = pow(3, 9)  # Number of states in the grid world (3^9 because 3 states per cell in a 3x3 grid)\n",
        "n_actions = 9  # Number of possible actions (one for each grid cell)\n",
        "\n",
        "# Initialize the Tic Tac Toe environment\n",
        "env = TicTacToeEnv()\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "# Q-table stores the Q-values for all states and actions\n",
        "Q_table = np.zeros((n_states, n_actions))\n",
        "\n",
        "# Define parameters for Q-learning\n",
        "learning_rate = 0.8  # How much we update the Q-value (alpha)\n",
        "discount_factor = 0.95  # How much we care about future rewards (gamma)\n",
        "exploration_prob = 0.2  # Probability of exploring instead of exploiting (epsilon)\n",
        "epochs = 1000000  # Number of training iterations\n",
        "\n",
        "done = False  # Flag to indicate if the game is over\n",
        "\n",
        "# Initialize the progress bar to show training progress\n",
        "progress_bar = tqdm(total=epochs, desc=\"Training Progress\", unit=\"epoch\", ncols=80)\n",
        "\n",
        "# Q-learning algorithm: the main training loop\n",
        "for epoch in range(epochs):\n",
        "    current_state = env.reset()  # Reset the environment to a random starting state\n",
        "    done = False  # Reset the 'done' flag for the new game\n",
        "\n",
        "    while not done:\n",
        "        # Choose an action using epsilon-greedy strategy\n",
        "        if np.random.rand() < exploration_prob:  # With probability epsilon, explore\n",
        "            action = np.random.randint(0, n_actions)  # Choose a random action\n",
        "        else:  # Otherwise, exploit the best known action\n",
        "            action = np.argmax(Q_table[current_state])  # Pick the action with the highest Q-value\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the Q-value for the current state and action\n",
        "        Q_table[current_state, action] += learning_rate * \\\n",
        "            (reward + discount_factor *  # Reward plus discounted future reward\n",
        "             np.max(Q_table[next_state]) -  # Max Q-value of the next state\n",
        "             Q_table[current_state, action])  # Subtract the old Q-value\n",
        "\n",
        "        # Move to the next state for the next iteration\n",
        "        current_state = next_state\n",
        "\n",
        "    # Update the progress bar after each epoch\n",
        "    progress_bar.update(1)\n",
        "\n",
        "# Save the Q-table to a file so we can use it later\n",
        "np.save('q_table.npy', Q_table)\n",
        "\n",
        "# Close the progress bar after training is complete\n",
        "progress_bar.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2XOEB9AyGVD-",
        "outputId": "12ed5e75-d8a4-4b88-c0a2-df9be52c25fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|█████████| 1000000/1000000 [03:43<00:00, 4468.81epoch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Script to Run Q-Learning Tic-Tac-Toe Model"
      ],
      "metadata": {
        "id": "AEdrcFd_dNfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the script to run our model. After training the model, please run this! We bet you'll struggle beating our model!"
      ],
      "metadata": {
        "id": "F5QYM3w-dQL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "# from ticTacToeQlearningEnv import TicTacToeEnv  # Import the TicTacToe environment (if it was in another file)\n",
        "import numpy as np  # Import NumPy for numerical operations\n",
        "\n",
        "# Load the pre-trained Q-table (a table the AI uses to decide the best moves)\n",
        "Q_table = np.load('q_table.npy')\n",
        "\n",
        "# Create an instance of the TicTacToe environment\n",
        "env = TicTacToeEnv(debug=True)  # Enable debug mode for extra information during gameplay\n",
        "\n",
        "# Initialize the game\n",
        "done = False  # This variable tracks if the game is over\n",
        "current_state = env.reset()  # Reset the game to start fresh\n",
        "env.print_board()  # Display the empty game board\n",
        "\n",
        "# Ask the player if they want to go first or let the computer go first\n",
        "if input(\"Do you want to go first (y/n)? \") == 'n':\n",
        "    # If the player says 'n', the computer makes the first move\n",
        "    action = np.argmax(Q_table[current_state])  # Choose the best move based on the Q-table\n",
        "    print(f\"The computer plays {action}\")  # Print the computer's move\n",
        "    current_state, reward, done, _ = env.step(action)  # Apply the computer's move\n",
        "    env.print_board()  # Display the updated board\n",
        "\n",
        "# Start the main game loop\n",
        "while not done:\n",
        "    # Ask the player for their move\n",
        "    i = int(input(\"What's your move (int 0-8)? \"))  # Get the player's move as an integer\n",
        "    while env.board[i] != 0:  # Check if the chosen position is valid (not already taken)\n",
        "        print(\"Invalid Move\")  # Inform the player if their move is invalid\n",
        "        i = int(input(\"What's your move (int 0-8)? \"))  # Ask for a new move\n",
        "\n",
        "    # Apply the player's move\n",
        "    current_state, reward, done, _ = env.step(i)  # Update the game with the player's move\n",
        "    env.print_board()  # Display the updated board\n",
        "\n",
        "    if not done:  # If the game is not over after the player's move\n",
        "        # Computer's turn\n",
        "        action = np.argmax(Q_table[current_state])  # Choose the best move using the Q-table\n",
        "        print(f\"The computer plays {action}\")  # Print the computer's move\n",
        "        current_state, reward, done, _ = env.step(action)  # Apply the computer's move\n",
        "        env.print_board()  # Display the updated board\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YPtJN0v8GrfK",
        "outputId": "c4acfded-165b-47a1-d83b-8c3383806aa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "Do you want to go first (y/n)? y\n",
            "What's your move (int 0-8)? 1\n",
            "  | X |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "The computer plays 0\n",
            "O | X |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "What's your move (int 0-8)? 2\n",
            "O | X | X\n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "The computer plays 3\n",
            "O | X | X\n",
            "---------\n",
            "O |   |  \n",
            "---------\n",
            "  |   |  \n",
            "What's your move (int 0-8)? 7\n",
            "O | X | X\n",
            "---------\n",
            "O |   |  \n",
            "---------\n",
            "  | X |  \n",
            "The computer plays 4\n",
            "O | X | X\n",
            "---------\n",
            "O | O |  \n",
            "---------\n",
            "  | X |  \n",
            "What's your move (int 0-8)? 6\n",
            "O | X | X\n",
            "---------\n",
            "O | O |  \n",
            "---------\n",
            "X | X |  \n",
            "The computer plays 5\n",
            "O Wins\n",
            "O | X | X\n",
            "---------\n",
            "O | O | O\n",
            "---------\n",
            "X | X |  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Deep Q-Learning"
      ],
      "metadata": {
        "id": "XH0CwrhFdkMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Q-Learning is similar to Q-Learning, but instead of a Q-table we have a neural network [3]. The input layer is based on the current state of the environment, and the output layer has one node representing the Q-value for each possible action [4].\n",
        "\n",
        "For example, let's look at tic tac toe. The input layer would consist of the nine squares, where a -1 might represent O, 0 for a blank square, and 1 for an X. The output layer would also consist of 9 outputs and would represent the Q-values for each possible action. One thing we want to point out is that it might be intuitive that there are 9 actions because some squares might be taken, but with RL we want to let the model learn the game and the rules without programming everything into the model. Instead of forcing the model to place objects on blank squares, we penalize the model for placing an X or O on another X or O.\n",
        "\n",
        "Another example to make this clear is a game like pong. In this case, we could take three consecutive frames, and combine them (this allows the model to see the motion of the ball rather than just the current position of the ball). If each frame is 1024 pixels, our input layer would be 3072 pixels. The output layer would be three nodes, one for right, one for left, and one for not moving.\n"
      ],
      "metadata": {
        "id": "uLKVOSHQdm1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://blog.mlq.ai/content/images/2019/07/deep-q-learning.png\" width=\"300\" height=\"200\">"
      ],
      "metadata": {
        "id": "0CkO_heHDThI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 The Loss Function"
      ],
      "metadata": {
        "id": "gH99KhuYdsVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Deep Q-Learning uses a neural network, we use a loss function to train it. The loss function compares our Q-value prediction and the Q-value “target”, and then uses gradient descent/backpropagation to approximate Q-values better. Our equation for the target Q value is really similar to the bellman equation: $$y_j = r_j + \\gamma \\max_a Q(S_{t+1}, a)$$ [3]\n",
        "\n",
        "\n",
        "In words, the Q value is the sum of the immediate reward and the discounted estimate of the maximum Q value of all possible next states.\n",
        "\n",
        "The loss function is: $$y_j - Q(\\phi_j, a_j; 0)$$ [3]\n",
        "\n",
        "\n",
        "Thinking about this intuitively, this simply means the target Q value minus the former Q value."
      ],
      "metadata": {
        "id": "utV_VK-JdvdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Sampling"
      ],
      "metadata": {
        "id": "pMAVTPpyd6aP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of our training algorithm is called sampling. We perform actions, and continuously store the observed experiences in a “replay “memory”. When we train the model, we’ll sample some of these experiences."
      ],
      "metadata": {
        "id": "FzXtxpLBd895"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1 Benefits of Sampling"
      ],
      "metadata": {
        "id": "FC2cc3Eud-3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are key benefits of sampling. First of all, it makes it a lot more efficient to train the model [3]. The traditional, slower method involves an agent interacting with an environment, getting experiences, learning from them, and then discarding them. This process is really inefficient, and is improved by sampling.  Experience replay allows the agent to learn from the same experiences multiple times.\n",
        "\n",
        "Additionally, a large problem when not using replay memory is forgetting previous experiences, known as catastrophic interference or catastrophic forgetting [3]. For example, if there are multiple levels to a game, the agent might forget how to beat earlier levels as it continues to play the game. Replay memory prevents this, because the agent may sample experiences from earlier on. In short, this makes it so the agent doesn’t just learn from recent experiences.\n",
        "\n",
        "Finally, the last major benefit of sampling is that it prevents the model from correlation observation sequences, and stabilizes learning [3]. For example, if we’re training a self-driving car, this will make it so the car doesn’t just learn from the most recent experiences like stopping at consecutive red lights, but rather it will learn from a wide range of experiences, like turning on a green light and yielding to a pedestrian.\n"
      ],
      "metadata": {
        "id": "CUnwbR6leCNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Double DQN using a Source and Target Network"
      ],
      "metadata": {
        "id": "5Yfo3NvHeQiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another large problem is that we are using the same model for the target and current Q-value, so the loss will be minimal and non optimal [5]. Every time we train the model and update the parameters, the Q-values and target - values shift. Eventually, it would converge at the optimal solution, but this process is really inefficient [5].\n",
        "\n",
        "This is sort of like a cop chasing a robber, but the robber keeps moving. Eventually, the robber will be caught, but this “chase” would be much less efficient than going straight to the final position of the robber.\n",
        "\n",
        "To solve this problem, we use a second neural network with fixed parameters to estimate TD Target. After multiple episodes, we copy the parameters from the source network to the target network.\n",
        "\n",
        "This also solves another problem, which is that the best action might not be the one with the highest Q value. This is especially true when we start training and don’t know any of the weights. By having two networks, we separate the action selection from the target Q-value generation. This is similar to how in Q-learning the updating of Q values is separate from choosing the next step (where there is an “exploration” aspect to choosing the next state). This separation allows our training to be faster and more stable.\n"
      ],
      "metadata": {
        "id": "UvLlzL_NeUFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Training of DDQN Tic-Tac-Toe Model"
      ],
      "metadata": {
        "id": "yuwWCQg4eXYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run these scripts to set up an environment and train a DDQN model of tic-tac-toe. We commented our code in detail to help users understand what we did and how they relate back to concepts we explained in this notebook. Please read the code and the comments in detail and refer to sections above to make sure you understand every line of code."
      ],
      "metadata": {
        "id": "mpouVCGyec82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TicTacToeEnv:\n",
        "    # Define constants for the players and blank spaces\n",
        "    X = 1  # Player X (uses 1)\n",
        "    O = -1  # Player O (uses -1)\n",
        "    BLANK = 0  # Empty spaces on the board\n",
        "\n",
        "    def __init__(self, debug=False):\n",
        "        # Initialize the game environment\n",
        "        self.reset()  # Start with a fresh board\n",
        "        self.debug = debug  # Enable or disable debug messages\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the board to an empty state\n",
        "        self.board = np.zeros(9, dtype=int)  # Create a 1D array with 9 empty spaces\n",
        "        self.current_player = self.X  # Player X always starts first\n",
        "        self.done = False  # The game is not over yet\n",
        "        self.winner = None  # No winner at the start\n",
        "        return self.get_state()  # Return the current state of the board\n",
        "\n",
        "    def step(self, action):\n",
        "        # Process the player's move (action)\n",
        "        reward: float  # This will hold the reward value\n",
        "\n",
        "        if self.done:\n",
        "            # Prevent moves if the game is already over\n",
        "            raise ValueError(\"Game is already done.\")\n",
        "\n",
        "        if self.board[action] != self.BLANK:\n",
        "            # Check if the move is invalid (space is already taken)\n",
        "            if self.debug:\n",
        "                print(f\"Computer made an invalid move. Game over\")\n",
        "            reward = -10  # Penalize for invalid move\n",
        "            self.done = True  # End the game\n",
        "            self.winner = -self.current_player  # Opponent wins if the move is invalid\n",
        "            return self.get_state(), reward, self.done, {}\n",
        "\n",
        "        # Place the current player's symbol on the board\n",
        "        self.board[action] = self.current_player\n",
        "\n",
        "        # Check if the current player has won the game\n",
        "        if TicTacToeEnv.check_winner(self.board, self.current_player):\n",
        "            if self.debug:\n",
        "                player = \"X\" if self.current_player == self.X else \"O\"\n",
        "                print(f\"{player} Wins\")  # Print which player won\n",
        "            self.done = True  # Mark the game as over\n",
        "            self.winner = self.current_player  # Set the winner\n",
        "            reward = 100  # Reward for winning\n",
        "        elif (self.board == self.BLANK).sum() == 0:\n",
        "            # Check if the board is full and it's a draw\n",
        "            if self.debug:\n",
        "                print(\"Draw\")  # Inform that the game is a draw\n",
        "            self.done = True  # Game is over\n",
        "            self.winner = self.BLANK  # No winner for a draw\n",
        "            reward = 0  # No reward for a draw\n",
        "        else:\n",
        "            # Valid move but the game continues\n",
        "            reward = 0  # Neutral reward for a valid move\n",
        "\n",
        "        # Switch to the other player's turn\n",
        "        self.current_player = -self.current_player\n",
        "\n",
        "        return self.get_state(), reward, self.done, {}\n",
        "\n",
        "    def check_draw(self):\n",
        "        # Check if there are no empty spaces left\n",
        "        return (self.board == self.BLANK).sum() == 0\n",
        "\n",
        "    def check_loss(self):\n",
        "        # Check if the current player can lose in the next move\n",
        "        for i in range(9):\n",
        "            if self.board[i] == self.BLANK:  # Try every empty space\n",
        "                self.board[i] = self.current_player  # Simulate a move\n",
        "                if TicTacToeEnv.check_winner(self.board, self.current_player):\n",
        "                    return True  # Found a potential loss\n",
        "                self.board[i] = self.BLANK  # Undo the move\n",
        "        return False\n",
        "\n",
        "    def get_state(self):\n",
        "        # Return the current state of the board\n",
        "        new_board = self.board.copy()  # Copy the board\n",
        "        new_board = np.append(new_board, self.current_player)  # Add the current player\n",
        "        new_board = new_board.reshape(10)  # Reshape into a 10x1 array\n",
        "        return self.board.copy()  # Return a copy of the board state\n",
        "\n",
        "    def check_winner(board, player):\n",
        "        # Check all possible winning combinations\n",
        "        wins = [\n",
        "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Horizontal rows\n",
        "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Vertical columns\n",
        "            [0, 4, 8], [2, 4, 6]  # Diagonal lines\n",
        "        ]\n",
        "        for combo in wins:\n",
        "            if all(board[i] == player for i in combo):\n",
        "                return True  # Player has a winning line\n",
        "        return False  # No winner found\n",
        "\n",
        "    def valid_actions(self):\n",
        "        # Return a list of all valid moves (empty spaces)\n",
        "        return [i for i in range(9) if self.board[i] == self.BLANK]\n",
        "\n",
        "    def print_board(self):\n",
        "        # Print the current state of the board\n",
        "        symbols = {self.X: \"X\", self.O: \"O\", self.BLANK: \" \"}  # Symbols for the players\n",
        "        for i in range(0, 9, 3):\n",
        "            # Create a row of the board\n",
        "            row = [symbols[self.board[i + j]] for j in range(3)]\n",
        "            print(\" | \".join(row))  # Print the row with separators\n",
        "            if i < 6:\n",
        "                print(\"---------\")  # Print a horizontal line between rows\n"
      ],
      "metadata": {
        "id": "mc0rntQnG5IC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "# from ticTacToeEnv import TicTacToeEnv\n",
        "\n",
        "# Define the Deep Q-Network (DQN) model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        # Define the neural network layers\n",
        "        self.fc1 = nn.Linear(input_dim, 27)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(27, 9)         # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(9, output_dim) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the network with ReLU activations\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the DQN Agent with a target network\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, buffer_size=5000, tau=0.1):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.lr = learning_rate  # Learning rate for optimization\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Initial exploration probability\n",
        "        self.epsilon_decay = epsilon_decay  # Decay rate for exploration\n",
        "        self.epsilon_min = epsilon_min  # Minimum exploration probability\n",
        "        self.memory = deque(maxlen=buffer_size)  # Replay buffer for experience replay\n",
        "\n",
        "        # Initialize the primary and target networks\n",
        "        self.model = DQN(state_dim, action_dim)  # Primary model\n",
        "        self.target_model = DQN(state_dim, action_dim)  # Target model\n",
        "        self.update_target_network()  # Synchronize target and primary models initially\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)  # Adam optimizer\n",
        "        self.tau = tau  # Soft update parameter for target network\n",
        "\n",
        "    def act(self, state):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # Explore: choose a random action\n",
        "            return np.random.choice(self.action_dim)\n",
        "        # Exploit: choose the action with the highest Q-value\n",
        "        q_values = self.model(torch.tensor(state, dtype=torch.float32))\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # Store the experience in the replay buffer\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        # Ensure there are enough samples in the memory to sample a batch\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Randomly sample a batch from memory\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward  # Initialize the target as the immediate reward\n",
        "            if not done:\n",
        "                # Add the discounted future reward from the target model\n",
        "                next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
        "                target = reward + self.gamma * torch.max(self.target_model(next_state_tensor)).item()\n",
        "\n",
        "            # Compute the Q-value for the current state and update the target\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            q_values = self.model(state_tensor)\n",
        "            target_f = q_values.clone().detach()  # Copy the Q-values\n",
        "            target_f[action] = target  # Update the Q-value for the taken action\n",
        "\n",
        "            # Compute the loss (difference between actual and target Q-values)\n",
        "            loss = nn.MSELoss()(q_values[action], torch.tensor(target, dtype=torch.float32))\n",
        "\n",
        "            # Backpropagate the loss and update the model\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon to reduce exploration over time\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_network(self):\n",
        "        # Copy the weights from the primary model to the target model\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def save_model(self, path):\n",
        "        # Save the model parameters to a file\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "\n",
        "# Training the Double DQN Agent\n",
        "episode_count = 1  # Number of episodes for training\n",
        "env = TicTacToeEnv()  # Initialize the Tic Tac Toe environment\n",
        "state_dim = 9  # State space dimension (3x3 grid)\n",
        "action_dim = 9  # Action space dimension (9 possible moves)\n",
        "batch_size = 100  # Batch size for replay\n",
        "\n",
        "# Create two agents for alternating turns\n",
        "agents = [DQNAgent(state_dim=state_dim, action_dim=action_dim, gamma=1),\n",
        "          DQNAgent(state_dim=state_dim, action_dim=action_dim, gamma=1)]\n",
        "\n",
        "# Train the agents over the episodes\n",
        "for episode in range(episode_count):\n",
        "    state = env.reset()  # Reset the environment to start a new game\n",
        "    total_reward = 0  # Initialize total reward for the episode\n",
        "    done = False  # Flag to indicate if the game is over\n",
        "    agent_turn = random.randint(0, 1)  # Randomly choose which agent starts\n",
        "\n",
        "    while not done:\n",
        "        # Get the current agent's action\n",
        "        action = agents[agent_turn].act(state)\n",
        "        # Perform the action and observe the result\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        # Store the experience in the agent's memory\n",
        "        agents[agent_turn].remember(state, action, reward, next_state, done)\n",
        "        # Update the state and total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        # Alternate turns between the agents\n",
        "        agent_turn = (agent_turn + 1) % 2\n",
        "\n",
        "    # Print progress every 5000 episodes\n",
        "    if not episode % 5000:\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    # Perform experience replay for both agents\n",
        "    agents[0].replay(batch_size=batch_size)\n",
        "    agents[1].replay(batch_size=batch_size)\n",
        "\n",
        "    # Update the target networks periodically\n",
        "    if episode % 10 == 0:\n",
        "        agents[0].update_target_network()\n",
        "        agents[1].update_target_network()\n",
        "\n",
        "# Save the trained models for both agents\n",
        "agents[0].save_model(\"agent_0_model.pth\")\n",
        "agents[1].save_model(\"agent_1_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "6tl-DZGGG7hR",
        "outputId": "bf050c4d-a195-4aa7-9b3c-be45fdb3bd66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b179f2f81ceb>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Create two agents for alternating turns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m agents = [DQNAgent(state_dim=state_dim, action_dim=action_dim, gamma=1),\n\u001b[0m\u001b[1;32m    109\u001b[0m           DQNAgent(state_dim=state_dim, action_dim=action_dim, gamma=1)]\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b179f2f81ceb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_dim, action_dim, learning_rate, gamma, epsilon, epsilon_decay, epsilon_min, buffer_size, tau)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Synchronize target and primary models initially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adam optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m  \u001b[0;31m# Soft update parameter for target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minductor_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShapeGuard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloorDiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPythonMod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIsNonOverlappingAndDenseIndicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCleanDiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloorToInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCeilToInt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'dev'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAtom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msingleton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAtomicExpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnevaluatedExpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumbers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRational\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInteger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumberSymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5 Script to Run DDQN Tic-Tac-Toe Model"
      ],
      "metadata": {
        "id": "TmfNZ0e6eeF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the script to run our model. After training the model, please run this. We did not get this model to fully converge, so there is a high chance you might be able to beat this model."
      ],
      "metadata": {
        "id": "NUwVzPq1Cn9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# from ticTacToeEnv import TicTacToeEnv  # Import the TicTacToe environment\n",
        "# from train import DQN  # Import the Deep Q-Network (DQN) model\n",
        "\n",
        "# Function to load a trained model from a file\n",
        "def load_model(model_path, state_dim, action_dim):\n",
        "    \"\"\"\n",
        "    Load the trained DQN model from the specified file.\n",
        "    Args:\n",
        "        model_path: Path to the saved model file\n",
        "        state_dim: The size of the state space (e.g., 9 for Tic-Tac-Toe)\n",
        "        action_dim: The size of the action space (e.g., 9 for Tic-Tac-Toe)\n",
        "    Returns:\n",
        "        model: The trained DQN model\n",
        "    \"\"\"\n",
        "    model = DQN(state_dim, action_dim)  # Create a new instance of the model\n",
        "    model.load_state_dict(torch.load(model_path))  # Load the saved model weights\n",
        "    model.eval()  # Set the model to evaluation mode (not training)\n",
        "    return model\n",
        "\n",
        "# Function to play a game against the AI\n",
        "def play_game(model_path):\n",
        "    \"\"\"\n",
        "    Play a game of Tic-Tac-Toe against the trained AI.\n",
        "    Args:\n",
        "        model_path: Path to the saved DQN model file\n",
        "    \"\"\"\n",
        "    env = TicTacToeEnv(debug=True)  # Create a new Tic-Tac-Toe game environment\n",
        "    state_dim = 9  # The game board has 9 spaces\n",
        "    action_dim = 9  # There are 9 possible actions (one for each space)\n",
        "    model = load_model(model_path, state_dim, action_dim)  # Load the trained model\n",
        "\n",
        "    state = env.reset()  # Reset the game to start fresh\n",
        "    done = False  # Keep track of whether the game is over\n",
        "\n",
        "    # Main game loop\n",
        "    while not done:\n",
        "        # AI's turn\n",
        "        action = torch.argmax(model(torch.tensor(state, dtype=torch.float32))).item()\n",
        "        # Use the model to predict the best action\n",
        "        print(f\"Computer placed in {action}\")  # Show the computer's move\n",
        "        state, reward, done, _ = env.step(action)  # Apply the AI's move to the game\n",
        "        env.print_board()  # Display the updated board\n",
        "        if done:  # Check if the game is over after the AI's move\n",
        "            break\n",
        "\n",
        "        # Player's turn\n",
        "        print(\"Your turn:\")\n",
        "        user_action = int(input(\"Enter your action (0-8): \"))  # Get player's move\n",
        "        # Validate the player's input\n",
        "        while user_action < 0 or user_action > 8 or env.board[user_action] != TicTacToeEnv.BLANK:\n",
        "            print(\"Invalid action. Please try again.\")  # Inform about invalid moves\n",
        "            user_action = int(input(\"Enter your action (0-8): \"))  # Ask for a valid move\n",
        "        state, reward, done, _ = env.step(user_action)  # Apply the player's move\n",
        "        env.print_board()  # Display the updated board\n",
        "\n",
        "# Run the game when the script is executed\n",
        "if __name__ == \"__main__\":\n",
        "    play_game(\"agent_0_model.pth\")  # Use the saved model file to play the game\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sweK8qIqHo2K",
        "outputId": "5beb4640-3fb1-4de6-8e07-63045a1a7241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-b8329f9a54c5>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computer placed in 7\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  | X |  \n",
            "Your turn:\n",
            "Enter your action (0-8): 1\n",
            "  | O |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  | X |  \n",
            "Computer placed in 7\n",
            "Computer made an invalid move. Game over\n",
            "  | O |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  | X |  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Sources"
      ],
      "metadata": {
        "id": "BWuC-AAAehxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] https://github.com/khpeek/Q-learning-Tic-Tac-Toe\n",
        "\n",
        "[2] https://www.youtube.com/watch?v=nOBm4aYEYR4&t=1519s\n",
        "\n",
        "[3] https://medium.com/@ardra4/tic-tac-toe-using-q-learning-a-reinforcement-learning-approach-d606cfdd64a3\n",
        "\n",
        "[4] https://www.youtube.com/watch?v=XjsY8-P4WHM\n",
        "\n",
        "[5] https://www.youtube.com/watch?v=aCEvtRtNO-M&t=4s\n",
        "\n",
        "[6] https://www.geeksforgeeks.org/what-is-reinforcement-learning/\n",
        "\n",
        "[7] https://www.ibm.com/think/topics/reinforcement-learning\n",
        "\n",
        "[8] https://www.youtube.com/watch?v=0MNVhXEX9to\n",
        "\n",
        "[9] https://www.youtube.com/watch?v=8JVRbHAVCws\n"
      ],
      "metadata": {
        "id": "fUxA55bDekNK"
      }
    }
  ]
}